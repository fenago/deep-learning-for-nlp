{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "from pickle import load\n", 
        "from numpy import argmax\n", 
        "from keras.preprocessing.text import Tokenizer\n", 
        "from keras.preprocessing.sequence import pad_sequences\n", 
        "from keras.models import load_model\n", 
        "from nltk.translate.bleu_score import corpus_bleu\n", 
        "\n", 
        "# load a clean dataset\n", 
        "def load_clean_sentences(filename):\n", 
        "\treturn load(open(filename, 'rb'))\n", 
        "\n", 
        "# fit a tokenizer\n", 
        "def create_tokenizer(lines):\n", 
        "\ttokenizer = Tokenizer()\n", 
        "\ttokenizer.fit_on_texts(lines)\n", 
        "\treturn tokenizer\n", 
        "\n", 
        "# max sentence length\n", 
        "def max_length(lines):\n", 
        "\treturn max(len(line.split()) for line in lines)\n", 
        "\n", 
        "# encode and pad sequences\n", 
        "def encode_sequences(tokenizer, length, lines):\n", 
        "\t# integer encode sequences\n", 
        "\tX = tokenizer.texts_to_sequences(lines)\n", 
        "\t# pad sequences with 0 values\n", 
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n", 
        "\treturn X\n", 
        "\n", 
        "# map an integer to a word\n", 
        "def word_for_id(integer, tokenizer):\n", 
        "\tfor word, index in tokenizer.word_index.items():\n", 
        "\t\tif index == integer:\n", 
        "\t\t\treturn word\n", 
        "\treturn None\n", 
        "\n", 
        "# generate target given source sequence\n", 
        "def predict_sequence(model, tokenizer, source):\n", 
        "\tprediction = model.predict(source, verbose=0)[0]\n", 
        "\tintegers = [argmax(vector) for vector in prediction]\n", 
        "\ttarget = list()\n", 
        "\tfor i in integers:\n", 
        "\t\tword = word_for_id(i, tokenizer)\n", 
        "\t\tif word is None:\n", 
        "\t\t\tbreak\n", 
        "\t\ttarget.append(word)\n", 
        "\treturn ' '.join(target)\n", 
        "\n", 
        "# evaluate the skill of the model\n", 
        "def evaluate_model(model, sources, raw_dataset):\n", 
        "\tactual, predicted = list(), list()\n", 
        "\tfor i, source in enumerate(sources):\n", 
        "\t\t# translate encoded source text\n", 
        "\t\tsource = source.reshape((1, source.shape[0]))\n", 
        "\t\ttranslation = predict_sequence(model, eng_tokenizer, source)\n", 
        "\t\traw_target, raw_src = raw_dataset[i]\n", 
        "\t\tif i < 10:\n", 
        "\t\t\tprint('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n", 
        "\t\tactual.append(raw_target.split())\n", 
        "\t\tpredicted.append(translation.split())\n", 
        "\t# calculate BLEU score\n", 
        "\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n", 
        "\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n", 
        "\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n", 
        "\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n", 
        "\n", 
        "# load datasets\n", 
        "dataset = load_clean_sentences('english-german-both.pkl')\n", 
        "train = load_clean_sentences('english-german-train.pkl')\n", 
        "test = load_clean_sentences('english-german-test.pkl')\n", 
        "# prepare english tokenizer\n", 
        "eng_tokenizer = create_tokenizer(dataset[:, 0])\n", 
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n", 
        "eng_length = max_length(dataset[:, 0])\n", 
        "# prepare german tokenizer\n", 
        "ger_tokenizer = create_tokenizer(dataset[:, 1])\n", 
        "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n", 
        "ger_length = max_length(dataset[:, 1])\n", 
        "# prepare data\n", 
        "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n", 
        "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n", 
        "# load model\n", 
        "model = load_model('model.h5')\n", 
        "# test on some training sequences\n", 
        "print('train')\n", 
        "evaluate_model(model, trainX, train)\n", 
        "# test on some test sequences\n", 
        "print('test')\n", 
        "evaluate_model(model, testX, test)"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}